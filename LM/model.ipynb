{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2729584",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk import trigrams\n",
    "from nltk import ConditionalFreqDist as CFD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc9b1e6",
   "metadata": {},
   "source": [
    "#### Part 1:  Building an N-Gram Language Model Using the NLTK Brown Corpus\n",
    "\n",
    "In this section, a statistical LM is created using the **Brown Corpus** from NLTK library. \n",
    "The data is preprocessed into all lower case words and adds the BOS and EOS for every sentence.\n",
    "\n",
    "The LM is built using a **Trigram Model:**\n",
    "\n",
    "$$\n",
    "P(w_i \\mid w_{i-2}, w_{i-1})\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca12aa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokens = []\n",
    "\n",
    "for sentence in brown.sents():\n",
    "    tokens.append(\"<s>\") # Add BOS and EOS tags for each sentence\n",
    "    tokens.append(\"<s>\") # Trigrams have two BOS tags\n",
    "\n",
    "    for words in sentence:\n",
    "        tokens.append(words.lower()) # Reduce vocabulary size The != the\n",
    "    tokens.append(\"</s>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e478bb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = []\n",
    "\n",
    "for word1, word2, word3 in trigrams(tokens):\n",
    "    trigram.append(((word1,word2),word3))\n",
    "\n",
    "trigram_table = CFD(trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10201d47",
   "metadata": {},
   "source": [
    "#### Part 2 - 3: Generate Predictions \"I am ... and Your are ...\"\n",
    "\n",
    "By using the Trigram model, simple predictions are made following `I am` and `You are`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c714bf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def predict_next_word(start):\n",
    "    start_words = start.lower().split()\n",
    "    word1 = start_words[-2]\n",
    "    word2 = start_words[-1]\n",
    "    \n",
    "    prediction = trigram_table[(word1,word2)]\n",
    "    \n",
    "    if len(prediction) == 0:\n",
    "        return \"</s>\"\n",
    "    \n",
    "    choices = list(prediction.keys())\n",
    "    weights = list(prediction.values())\n",
    "    \n",
    "    return random.choices(choices, weights=weights)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "501399a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(start, max_len=15):\n",
    "    words = start.lower().split()\n",
    "    \n",
    "    if len(words) == 1:\n",
    "        words = [\"<s>\" , words[0]]\n",
    "    elif len(words) == 0:\n",
    "        words = [\"<s>\" , \"<s>\"]\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        start_now = \" \".join(words)\n",
    "        next_word = predict_next_word(start_now)\n",
    "        \n",
    "        if next_word == \"</s>\":\n",
    "            break\n",
    "        \n",
    "        words.append(next_word)\n",
    "        \n",
    "        if next_word == \"</s>\":\n",
    "            break\n",
    "    \n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b86b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "PUNCT_TO_REMOVE = {\"''\", \"``\", \"--\"}\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    words = sentence.split()\n",
    "\n",
    "\n",
    "    words = [w for w in words if w not in PUNCT_TO_REMOVE]\n",
    "\n",
    "    # 2. remove punctuation after prefix (e.g., \"i am '' happy\")\n",
    "    if len(words) > 2 and words[2] in PUNCT_TO_REMOVE:\n",
    "        words.pop(2)\n",
    "\n",
    "    text = \" \".join(words)\n",
    "\n",
    "    # 3. remove space before punctuation\n",
    "    text = re.sub(r\"\\s+([.,?!;:])\", r\"\\1\", text)\n",
    "\n",
    "    # 4. remove duplicate punctuation\n",
    "    text = re.sub(r\"([.,?!])\\1+\", r\"\\1\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c282e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are a nonresident alien and outside ; ;\n",
      "you are of equal parts of the city council and mr. sharpe first saw her except to\n",
      "you are normally obligated to make a wish and turn a pool .\n",
      "you are '' ? ?\n",
      "you are the same american catholic higher education more available through membership in the misty marshlands and\n",
      "you are turning over the babies were getting on the floor .\n",
      "you are conscientious , selfless efforts deserve the nation's backlog of an army blanket , working ,\n",
      "you are thinking about the only one who had the situation .\n",
      "you are sitting indolently on the side .\n",
      "you are thwarted if you insist that there would still be the statue because of the national\n"
     ]
    }
   ],
   "source": [
    "sentence = \"You are\"\n",
    "you_are_sentences = []\n",
    "\n",
    "for _ in range(10):\n",
    "    \n",
    "    you_are_sentences.append(generate_sentence(sentence))\n",
    "    \n",
    "for i in you_are_sentences:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "1a0780a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am simply too old '' .\n",
      "i am highly privileged today to ask voters whether they would sleep , the imperative operations properly\n",
      "i am for it is the last of the human replaces amplifier af in a notably condescending\n",
      "i am concerned , the dramatist once needed an idea a chain of being known and worthy\n",
      "i am happy '' bodybuilder -- looks as if this woman repeatedly complained she was not hardy\n",
      "i am '' , `` for christ's sake , stay thou with me .\n",
      "i am innocent '' .\n",
      "i am for it to the u.s. army .\n",
      "i am reliably given to slaves in canada , brazil , the first decision was a baby\n",
      "i am .\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I am\"\n",
    "i_am_sentences = []\n",
    "\n",
    "for _ in range(10):\n",
    "    i_am_sentences.append(generate_sentence(sentence))\n",
    "    \n",
    "for i in i_am_sentences:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afda6f3a",
   "metadata": {},
   "source": [
    "#### Part 4: Calculate the probabilities of the predicted words\n",
    "\n",
    "To compute the probabilities of a sentence, use the **Trigram Formula** \n",
    "\n",
    "$$\\prod_{i=1}^{n} P(word_i|word_{i-2},word_{i-1}) = \\prod_{i=1}^{n} \\frac{count(word_{i-2}, word_{i-1}, word_i)}{count(word_{i-2}, word_{i-1})}$$\n",
    "\n",
    "Note: Laplace smoothing (Add 1) was added, incase any raw counts have `zero`\n",
    "\n",
    "*modified bigram formula with laplace smoothing:*\n",
    "$$P_{LAP}(w_i | h) = \\frac {Count(h,w_i) +1}{Count(h) + |V|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "191fa5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = len(set(tokens))\n",
    "\n",
    "def trigram_probability(w1, w2, w3):\n",
    "    numerator = trigram_table[(w1, w2)][w3] + 1    \n",
    "    denominator = trigram_table[(w1, w2)].N() + V\n",
    "\n",
    "    return numerator / denominator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2d777e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentence_probability(sentence):\n",
    "    words = sentence.lower().split()\n",
    "\n",
    "    words.insert(0, \"<s>\")\n",
    "    words.insert(0, \"<s>\")\n",
    "    words.append(\"</s>\")\n",
    "\n",
    "    probability = 1.0\n",
    "\n",
    "    for i in range(2, len(words)):\n",
    "        w1 = words[i-2]\n",
    "        w2 = words[i-1]\n",
    "        w3 = words[i]\n",
    "\n",
    "        probability *= trigram_probability(w1, w2, w3)\n",
    "\n",
    "    return probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b6f12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.86e-29\n",
      "you are taking it : </s>\n",
      "--------------------------------\n",
      "1.44e-49\n",
      "you are not guided by logic or common sense . </s>\n",
      "--------------------------------\n",
      "4.54e-41\n",
      "you are really an excess of two . </s>\n",
      "--------------------------------\n",
      "1.78e-73\n",
      "you are budget-wise , when he was on a trip which could bring enormous pressure to correct\n",
      "--------------------------------\n",
      "2.19e-69\n",
      "you are very tactful , do you try scaring that kid out there '' ? ? </s>\n",
      "--------------------------------\n",
      "3.26e-51\n",
      "you are going to live in boxcars in the universe . </s>\n",
      "--------------------------------\n",
      "5.09e-19\n",
      "you are . </s>\n",
      "--------------------------------\n",
      "1.26e-42\n",
      "you are staring , open-mouthed and blushing . </s>\n",
      "--------------------------------\n",
      "3.40e-32\n",
      "you are being pulled down . </s>\n",
      "--------------------------------\n",
      "5.07e-47\n",
      "you are that production topped the sextet brilliantly . </s>\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "you_are_sentence_prob = []\n",
    "\n",
    "for sentence in you_are_sentences: \n",
    "    P = compute_sentence_probability(sentence)\n",
    "    print(f'{P:.2e}')\n",
    "    print(sentence)\n",
    "    print(\"--------------------------------\")\n",
    "    you_are_sentence_prob.append((sentence, P))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69719cd7",
   "metadata": {},
   "source": [
    "#### Part 5: Computer Perplexity of the model\n",
    "\n",
    "`Perplexity` will measure how suprised the model is by a given sentence. A `high` perplexity the model is very suprised, a `low` perplexity the model is confident in it's predictions\n",
    "\n",
    "$$PP(sentence) = P(sentence)^{-1/n}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f4b4eaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(sentence):\n",
    "    p = compute_sentence_probability(sentence)\n",
    "\n",
    "    words = sentence.lower().split()\n",
    "    words.insert(0,\"<s>\")\n",
    "    words.insert(0,\"<s>\")\n",
    "    words.append(\"</s>\")\n",
    "\n",
    "    return p ** (-1.0/len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "3faa80d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('you are taking it : </s>', 3.8582585078121273e-29)\n",
      "1435.712993420087\n",
      "('you are not guided by logic or common sense . </s>', 1.4369841413167151e-49)\n",
      "3081.437974432746\n",
      "('you are really an excess of two . </s>', 4.53986732778117e-41)\n",
      "2300.9811977737995\n",
      "('you are budget-wise , when he was on a trip which could bring enormous pressure to correct', 1.77885714858967e-73)\n",
      "4340.032146483798\n",
      "(\"you are very tactful , do you try scaring that kid out there '' ? ? </s>\", 2.1949054114459663e-69)\n",
      "2709.7499037779553\n",
      "('you are going to live in boxcars in the universe . </s>', 3.25884648235167e-51)\n",
      "2321.644608851134\n",
      "('you are . </s>', 5.088405903119621e-19)\n",
      "410.5305344321058\n",
      "('you are staring , open-mouthed and blushing . </s>', 1.263936642654907e-42)\n",
      "3101.150843171676\n",
      "('you are being pulled down . </s>', 3.4049293431741554e-32)\n",
      "1402.1329315172488\n",
      "('you are that production topped the sextet brilliantly . </s>', 5.073300170879983e-47)\n",
      "3640.252388260482\n"
     ]
    }
   ],
   "source": [
    "# you are\n",
    "for sentence in you_are_sentence_prob:\n",
    "    print(sentence)\n",
    "    print(compute_perplexity(sentence[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1256b392",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
